#install packages.
!pip install gensim
import gensim
import re
import requests

!pip install sumy
import sumy

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer
import nltk
nltk.download('punkt')

#open files.
with open('/content/Douglass Article 1 (1).txt') as f:
    text1 = f.readlines()
with open('/content/Douglass Article 2.txt') as f:
    text2 = f.readlines()

#cycle through data.
article1 = ''
for line in text1: 
  article1 +=line

article2 = ''
for line in text2: 
  article2 +=line

#remove certain characters and extra spaces.
article1 = re.sub('[^a-zA-Z\.]', ' ', article1)
article1 = re.sub(r'\s+', ' ', article1)

article2 = re.sub('[^a-zA-Z\.]', ' ', article2)
article2 = re.sub(r'\s+', ' ', article2)

print(article1)
print(article2)

#remove stopwords
article1_parsed = PlaintextParser.from_string(article1,Tokenizer('english'))
article2_parsed = PlaintextParser.from_string(article2,Tokenizer('english'))

#creat summarizer. that breaks the first article into 3 sentences
lex_rank_summarizer = LexRankSummarizer()
lexrank_summary = lex_rank_summarizer(article1_parsed.document,sentences_count=3)
#print summary.
lexrank_summary

#create summarizer. that breaks the first article into 3 sentences.
lex_rank_summarizer = LexRankSummarizer()
lexrank_summary = lex_rank_summarizer(article2_parsed.document,sentences_count=3)

#print summary 
lexrank_summary
